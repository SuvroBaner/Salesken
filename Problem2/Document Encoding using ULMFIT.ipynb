{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Encoding using <a href = 'https://arxiv.org/pdf/1801.06146.pdf'>ULMFIT</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach to find the similarity between the documents, we will encode the document into a high-dimensional vector representation, by taking the output of last timestamp of language model encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../list_of_sentences', 'r+') as f:\n",
    "    documents = f.readlines()\n",
    "for i in range(len(documents)):\n",
    "    documents[i] = documents[i][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good morning',\n",
       " 'how are you doing ?',\n",
       " 'the weather is awesome today',\n",
       " 'samsung',\n",
       " 'good afternoon',\n",
       " 'baseball is played in the USA',\n",
       " 'there is a thunderstorm ',\n",
       " 'are you doing good ?',\n",
       " 'The polar regions are melting\"',\n",
       " 'apple',\n",
       " 'nokia',\n",
       " 'cricket is a fun game',\n",
       " 'the climate change is a problem']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/anubhav/.fastai/data/imdb/unsup'),\n",
       " PosixPath('/home/anubhav/.fastai/data/imdb/test'),\n",
       " PosixPath('/home/anubhav/.fastai/data/imdb/train'),\n",
       " PosixPath('/home/anubhav/.fastai/data/imdb/imdb.vocab'),\n",
       " PosixPath('/home/anubhav/.fastai/data/imdb/tmp_clas'),\n",
       " PosixPath('/home/anubhav/.fastai/data/imdb/README'),\n",
       " PosixPath('/home/anubhav/.fastai/data/imdb/tmp_lm')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB) #this will download a 176 MB tgz file(only downloads once)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're using imdb's movie reviews dataset's vocabulary, for encoding the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = (TextList.from_folder(path)\n",
    "            .filter_by_folder(include=['train', 'test', 'unsup']) \n",
    "            .split_by_rand_pct(0.01, seed=42)\n",
    "            .label_for_lm()           \n",
    "            .databunch(bs=32, num_workers=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.save('lm_db_movie.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_lm.vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture chosen is <a href = \"https://docs.fast.ai/text.models.html#AWD_LSTM\">AWD-LSTM</a> pretrained on wikitext-103 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, AWD_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('learn_similar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This language model learner object has 2 sub-nets Encoder and a Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(60000, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1152, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1152, 1152, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1152, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=60000, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The encoding of our document will be a 400-dim vector, taken as from the last timestamp of the language model encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_item(learn, doc):\n",
    "    xb, yb = learn.data.one_item(doc)\n",
    "    return xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_doc(learn, doc):\n",
    "    xb = get_one_item(learn, doc)\n",
    "    lstm_encoder = learn.model[0]\n",
    "    lstm_encoder.reset()\n",
    "    with torch.no_grad():\n",
    "        out = lstm_encoder.eval()(xb)\n",
    "    return out[0][2][0][-1].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    we take the vector representation of the documents in the variable document_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_matrix = []\n",
    "for doc in documents:\n",
    "    doc_vector = encode_doc(learn, doc)\n",
    "    document_matrix.append(doc_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 400)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_matrix = np.array(document_matrix)\n",
    "document_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(documents, document_matrix):\n",
    "    '''\n",
    "    find the similar documents based on the cosine distance of two vectors\n",
    "    '''\n",
    "    similar_list = []\n",
    "    for i in range(len(documents)):\n",
    "        sim = cosine_similarity(document_matrix[i:i+1], document_matrix)[0]\n",
    "#         print((sim))\n",
    "        indexes = [i for i,s in enumerate(sim) if np.logical_and(s<0.99 , s>0.)]#np.argwhere(np.logical_and(sim>0., sim<1.))\n",
    "#         print(indexes)\n",
    "        idx_sim_pairs = {}\n",
    "        for idx in indexes:\n",
    "            idx_sim_pairs[int(idx)] = sim[idx]\n",
    "        idx_sim_pairs = {k: v for k, v in sorted(idx_sim_pairs.items(), key=lambda item: item[1], reverse=True)}\n",
    "#         print(idx_sim_pairs)\n",
    "        sim_sentences = [documents[i]]\n",
    "        sim_sentences.extend([documents[i] for i in list(idx_sim_pairs.keys())[:2]])\n",
    "        similar_list.append(sim_sentences)\n",
    "        \n",
    "    return similar_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['good morning', 'good afternoon', 'apple'],\n",
       " ['how are you doing ?',\n",
       "  'are you doing good ?',\n",
       "  'the weather is awesome today'],\n",
       " ['the weather is awesome today',\n",
       "  'the climate change is a problem',\n",
       "  'how are you doing ?'],\n",
       " ['samsung', 'apple', 'good afternoon'],\n",
       " ['good afternoon', 'good morning', 'samsung'],\n",
       " ['baseball is played in the USA',\n",
       "  'the weather is awesome today',\n",
       "  'cricket is a fun game'],\n",
       " ['there is a thunderstorm ',\n",
       "  'the weather is awesome today',\n",
       "  'the climate change is a problem'],\n",
       " ['are you doing good ?',\n",
       "  'how are you doing ?',\n",
       "  'the weather is awesome today'],\n",
       " ['The polar regions are melting\"',\n",
       "  'the weather is awesome today',\n",
       "  'the climate change is a problem'],\n",
       " ['apple', 'samsung', 'nokia'],\n",
       " ['nokia', 'apple', 'good afternoon'],\n",
       " ['cricket is a fun game',\n",
       "  'the weather is awesome today',\n",
       "  'baseball is played in the USA'],\n",
       " ['the climate change is a problem',\n",
       "  'the weather is awesome today',\n",
       "  'cricket is a fun game']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_similar(documents, list(document_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here as compared to TF-IDF it is able to find similarity in the below example <br>\n",
    "\n",
    "['cricket is a fun game',\n",
    "  'the weather is awesome today',\n",
    "  'baseball is played in the USA']\n",
    "  \n",
    "\n",
    "as it contains **'baseball is played in the USA'**, somewhat related to **sports**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good morning',\n",
       " 'how are you doing ?',\n",
       " 'the weather is awesome today',\n",
       " 'samsung',\n",
       " 'good afternoon',\n",
       " 'baseball is played in the USA',\n",
       " 'there is a thunderstorm ',\n",
       " 'are you doing good ?',\n",
       " 'The polar regions are melting\"',\n",
       " 'apple',\n",
       " 'nokia',\n",
       " 'cricket is a fun game',\n",
       " 'the climate change is a problem']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Scope of improvements\n",
    "\n",
    "* if we had a big corpus, where we can fine-tune our language-model for our text documents, we would be able to group documents together with better accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
